ðŸ§­ Observability & Monitoring Runbook

Version: 2.0
Last Updated: {{ date }}
Author: Infrastructure & DevOps Team â€“ Project Athlete 360
Reviewed By: Security & Platform Reliability Engineering (PRE)


---

ðŸŽ¯ Objective

This runbook defines observability, monitoring, and incident-response procedures for the Project Athlete 360 backend and cloud infrastructure.

It ensures:

End-to-end visibility across all services and data pipelines.

Early anomaly detection through metrics, logs, and traces.

Faster mean-time-to-detect (MTTD) and mean-time-to-recover (MTTR).

Audit-grade telemetry retention for compliance and security forensics.



---

ðŸ“š Scope

Applies to:

API gateways and backend microservices (Node/Express)

Worker clusters (BullMQ, telemetry, notification)

PostgreSQL (Neon / RDS)

Redis

Object storage (S3 or MinIO)

AI & analytics microservices

Observability stack (Prometheus, Grafana, Loki, Jaeger/Tempo, Alertmanager)

Kubernetes or Docker environments



---

ðŸ§© Observability Stack Overview

Layer	Component	Function	Location

Metrics	src/lib/core/metrics.ts	Prometheus counters & histograms	API / Workers
Tracing	src/lib/core/tracing.ts	OpenTelemetry spans & trace context	All
Telemetry	src/lib/telemetry.ts + telemetry.worker.ts	Metric aggregation, threshold alerts	Workers
Logging	src/logger.ts	JSON + colorized structured logs	Global
Health	systemHealth.service.ts / systemMonitor.ts	System status aggregation	Core
Visualization	Grafana + Tempo	Dashboards, trace explorer	infra/observability/grafana
Alerting	Alertmanager	Multi-channel alert dispatch	infra/observability/alertmanager



---

âš™ï¸ Key Observability Domains

1. Metrics (Quantitative Performance)

Collected via prom-client and OpenTelemetry Metrics API.

Categories

API latency histograms and error rate

Queue health (active/waiting/failed)

Worker execution time & success ratio

Resource metrics (CPU, memory, disk, DB pool)

AI inference latency

Business KPIs (active users, plan usage, overages)


Endpoints

/metrics â€” Prometheus scrape

/admin/system/status â€” JSON health snapshot


Retention

Prometheus TSDB (14â€“30 days)

Optional offload to Grafana Cloud or VictoriaMetrics



---

2. Logs (Structured Events)

All logs follow structured JSON format with traceId correlation.

Logging Guidelines

Each request includes x-request-id or traceId.

Logs include service, userId, route, latency, status.

Errors log stack traces (non-prod) + exception fingerprint (prod).

Security/audit events go to auditService.


Storage

Local: logs/*.log

Centralized: Loki or S3 archive (90-day retention)

Error tracking: Sentry integration



---

3. Traces (Distributed Context)

Traces created via tracing-http.ts middleware and OpenTelemetry SDK.

Propagation

Follows W3C Trace Context (traceparent, tracestate)

Auto-injected via Express middleware

Queues & async jobs preserve parent span where possible


Exporters

Jaeger (default) â€” Self-hosted trace store

Grafana Tempo (optional) â€” via OTLP/HTTP exporter

Sampling rate: 1% (prod), 10% (staging), 100% (dev)



---

4. Health Monitoring (Active Checks)

Driven by systemHealth.service.ts.

Checks

DB latency & reachability

Redis queue status

Worker job activity

AI service response

S3 object read/write probe

System resource usage (CPU/memory/disk)


Frequency: 60 s (default)
Endpoints:

/health â€” readiness

/admin/system/status â€” extended report


Alerting:
Triggered via telemetry worker â†’ Alertmanager â†’ Email / Slack


---

5. Telemetry & Alerts

Alerts use threshold-based detection and correlation.

Example Thresholds

Metric	Condition	Severity	Action

CPU usage	>85 %	warning	Log + telemetry notice
Memory usage	>90 %	critical	Super-admin alert
Queue backlog	>25 jobs	warning	Worker review
DB latency	>300 ms	degraded	Performance audit
Error rate	>2 %	warning	DevOps notify
Trace drop rate	>10 %	critical	Restart otel agent


Alert Channels

Email (admins@projectathlete360.com)

Slack #infra-alerts

Sentry incidents

PagerDuty (critical only)



---

ðŸ§° Troubleshooting & Diagnostics

Symptom	Possible Cause	Mitigation

/metrics empty	Prometheus client not registered	Restart API or import metrics.ts
Grafana panels blank	Data source misconfig	Verify Prometheus endpoint
High /metrics latency	Overloaded collectors	Increase scrape interval
Alerts missing	Telemetry worker paused	Restart telemetry.worker
â€œAI degradedâ€	Endpoint timeout	Check ai.bootstrap logs
Trace gaps	Context lost in async	Wrap async jobs in trace.wrap()



---

ðŸ§  Escalation & Incident Response

Severity	Response Time	Responsible	Notification

P1 â€“ Critical	â‰¤15 min	Infra Lead	PagerDuty + Slack
P2 â€“ Degraded	â‰¤1 h	SRE Team	Email + Slack
P3 â€“ Warning	â‰¤4 h	DevOps On-Call	Email
P4 â€“ Info	â‰¤24 h	Scheduled Review	N/A


Incident Steps

1. Verify alert in Grafana/Alertmanager.


2. Confirm affected service via health dashboard.


3. Retrieve trace IDs from Jaeger for failing requests.


4. Cross-reference with logs and metrics for correlation.


5. Patch, redeploy, or scale service as needed.


6. Record incident summary in IncidentRegistry.md.




---

ðŸ”’ Security & Compliance Observability

Secrets access tracked via SecretsManager audit events.

All /admin and /metrics routes protected by token or mTLS.

Sensitive fields (password, apiKey, token) redacted before logging.

Logs retained for 90 days, then archived to encrypted S3 bucket.

Changes in observability configuration audited via auditService.



---

ðŸ§© Maintenance & Validation

Task	Frequency	Owner	Notes

Validate /metrics scrape	Weekly	DevOps	Test Prometheus target
Verify Grafana dashboards	Weekly	SRE	Ensure live data
Full health check	Daily	Automated Worker	runFullHealthCheck()
Review alert logs	Daily	Infra Lead	via systemHealth.service
Trace audit & latency check	Monthly	Engineering	Investigate slow spans
Prometheus retention check	Quarterly	DevOps	TSDB cleanup
Rotate alert credentials	Quarterly	Security	PagerDuty/Slack tokens



---

ðŸŒ Multi-Region & Failover

Prometheus instances replicated per region (in.us-east, in.eu-west).

Grafana configured with unified data sources and per-region filters.

Failover triggers when regional Prometheus becomes unhealthy for >3 min.

Backup dashboards auto-synced from docs/dashboards/.

Loki logs mirrored to S3 (cross-region replication enabled).



---

ðŸ“Š Observability KPIs

KPI	Target	Description

Metrics uptime	â‰¥ 99.9 %	/metrics endpoint availability
Trace coverage	â‰¥ 95 %	% of API requests traced
Log completeness	â‰¥ 99 %	Events with traceId correlation
Alert delivery latency	< 60 s	Time between trigger and notification
Mean time to detect (MTTD)	< 10 min	Detection latency for major incidents
Mean time to recover (MTTR)	< 30 min	Resolution latency



---

ðŸ§­ Recovery Playbook

If observability stack fails:

1. Restart telemetry and worker containers.


2. Run docker-compose -f docker-compose.observability.yml up -d.


3. Verify /metrics endpoint returns valid output.


4. Access Prometheus /targets page â†’ ensure healthy state.


5. Check Grafana logs (docker logs grafana).


6. Reconnect Jaeger/Tempo exporter if traces missing.


7. Notify Infra Lead if recovery > 30 min or data loss > 15 min window.




---

âœ… Acceptance Criteria

/metrics and /health endpoints reachable in all environments.

Prometheus scrapes succeed every 15 s.

Jaeger/Tempo traces â‰¥ 95 % of API requests.

Super-admin dashboard live & accurate.

Alertmanager sends notifications within SLA.

All observability changes logged via audit events.



---

ðŸ“˜ References

Prometheus Docs

OpenTelemetry Node SDK

Grafana Observability Stack

Sentry Error Tracking

AWS Secrets Manager & Vault Integration